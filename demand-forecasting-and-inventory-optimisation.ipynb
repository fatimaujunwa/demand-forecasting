{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":826548,"sourceType":"datasetVersion","datasetId":435331},{"sourceId":7644881,"sourceType":"datasetVersion","datasetId":4456095}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting the Future of Product Sales and Crafting the Optimal Inventory Strategy\n\n","metadata":{}},{"cell_type":"markdown","source":"\n<h1>Introduction</h1>\n","metadata":{}},{"cell_type":"markdown","source":"<p>Navigating the ever-shifting landscape of customer demand is a core challenge that supply chain professionals face. Supply chain professionals often combine their expert intuition with some basic statistical techniques to infer the demand for products based on historical data. This process is relatively straightforward when dealing with smaller product categories and stable demand patterns. However, As businesses grow and encounter larger product categories with constantly shifting consumer demands, this traditional demand forecasting approach may not scale.\n</p>\n\n<p>\nIn this notebook, we will explore a more modern and scalable alternative – a data-driven, programmatic approach to demand forecasting. We will build a demand forecasting model with Python. We will also use this model for inventory optimization, covering concepts like reorder points, safety stock, and economic order quantity (EOQ).\n</p>\n<p>\nThroughout this project, we will address two key business questions:\n</p>\n<ol>\n<li>\n<em>What is the demand forecast for the top-selling product in the next 24 months?</em>\n</li>\n<li>\n<em>What is the optimal inventory level for the product?</em>\n</li>\n</ol>\n<p>\nJoin me on this exploration as we seek answers, and feel free to share your thoughts and feedback on my approach.\n</p>\n<p>You can find the source code for this project at my <a href=\"https://github.com/Simontagbor/supply-chain-demand-forecast\">github page.</a> You can also find the Jupyter notebook and the dataset on my <a href=\"https://www.kaggle.com/simontagbor/british-airways-predictive-ml\">kaggle page</a>.</p>\n</div>\n\n","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"<h1>Project Outline</h1>\n<p>For this project, I completed the following tasks:</p>\n\n- [x] Performed Exploratory Data Analysis.\n- [x] Cleaned and Prepared the data for modeling.\n- [x] Conducted Time Series Modeling With Prophet.\n- [x] Evaluated the model performance.\n- [x] Interpret the model results and answer the business questions.\n","metadata":{}},{"cell_type":"markdown","source":"## Problem Statement\n<div class=\"card\">\n  <p>Large product categories and constantly shifting consumer demand patterns introduce a scaling challenge for traditional demand forecasting techniques. There is a need for an approach that reduces the level of guesswork and reduces the avoidable costly outcomes of poor inventory optimizations.</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Prerequisites\n<p> To follow along you need to have a basic understanding of the following:</p>\n<ul>\n<li>Python Programming. You can take a quick introduction to Python by following this <a href=\"https://www.pythonforbeginners.com/python-tutorial\">tutorial</a></li>\n<li>Time Series Analysis. You can take a quick <a href=\"https://zerotomastery.io/blog/time-series-forecasting-with-facebook-prophet/\">introduction to Time Series Analysis</a> </li>\n<li>Understanding of <a href=\"https://www.shipbob.com/inventory-management/inventory-optimization/\">Inventory Optimization techniques.</a> will be a plus</li>\n</ul>\n\n<p>If you're already familiar with these concepts, you're good to go!</p>\n","metadata":{}},{"cell_type":"markdown","source":"\n\n<img src=\"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExaGpxN21laHVsY3Ixc2tsYmFvcGpmMjR1aHo4MXBsejY4azlncHpndCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/UxTBfyPE5JwRQbPCgU/giphy.gif\" alt=\"gif\" style=\"width: 40%; height: 100%;\">\t","metadata":{}},{"cell_type":"markdown","source":"## Project Dependencies\n","metadata":{}},{"cell_type":"code","source":"#| code-fold: true\n#| code-summary: \"Show the code\"\n# import project libraries\nimport pandas as pd\nimport numpy as np # for linear algebra\nimport math # for math operations \n\nimport seaborn as sns # for plotting\n\n# handling files\nimport os \nimport sys \n\n# data preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n\n# Model Building and Fitting\nfrom sklearn.ensemble import RandomForestClassifier\nfrom prophet import Prophet\n\n\n\n# Model Evaluation and Tuning\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt # for plotting\nimport squarify # for tree maps","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n<p>\nThe working <a href=\"https://www.kaggle.com/datasets/shashwatwork/dataco-smart-supply-chain-for-big-data-analysis\">dataset</a> contains entries of customer demand information The data contains 53 features (columns)\n</p>\n<p>\nTo understand the data, let's perform some exploratory data analysis. We will  use the following techniques:\n</p>\n<ul class=\"task-list\">\n<li><input type=\"checkbox\" checked=\"\">Visual inspection of data.</li>\n<li><input type=\"checkbox\" checked=\"\">Exploratory Data Visualizations. (Univariate and Bivariate)</li>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"## Visual Inspection of Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"data/DataCoSupplyChainDataset.csv\", encoding=\"ISO-8859-1\")\ndf.head(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>To explore the spread of the data, we will use the describe() method to get the summary statistics of the data.</p>","metadata":{}},{"cell_type":"code","source":"# retrieve the number of columns and rows\ndf.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<details><summary>Click to see some Notable Observation of the Data</summary>\n<ol type=\"1\">\n<li><p>Aproximately <code>55%</code> of orders had late delivery risks.</p></li>\n<li><p>Aproximately <code>75%</code> of products cost <code>$199.99</code></p></li>\n<li><p>All the products are available.</p></li>\n<li><p>75% of customers bought goods worth at least <code>$247.40</code></p></li>\n</ol>\n<p>\nFurther inspection of the data will help us understand the data better.\n</p>\n</details>","metadata":{}},{"cell_type":"markdown","source":"\n## Data Preprocessing\n\n<p>\nwe will focus on historical sales data, and product attributes like; stock level, and product category, we will also analyze the impact of other variables that contribute to demand patterns including geographic factors, customer segments and lead time.\n</p>\n\n### Preprocessing Tasks\n\n- [x] Drop irrelevant columns\n- [x] Drop rows with missing values\n- [x] Create new features\n- [x] Convert categorical features to numerical features\n\n<p>Based on the above, we will drop the majority of the columns that are not relevant for forecasting the demand and extract new features from the existing columns\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"\n#### Drop Irrelevant Columns\n","metadata":{}},{"cell_type":"code","source":"#| code-fold: true\n#| code-summary: \"Show the code\"\n\n# drop irrelevant columns\ndef drop_columns(df, columns_to_drop):\n    try:\n        df = df.drop(columns=columns_to_drop)\n        print(f\"{len(columns_to_drop)} columns dropped successfully. Number of columns remaining: {len(df.columns)}\")\n        return df\n    except KeyError as e:\n        print(f\"\"\"Column(s): {e} not found in dataframe.\n              \n            No columns dropped.\n            Please Check that the column names are correct.\"\"\")\n        return df\n\n# Specify the columns to keep\ncolums_to_keep = ['Days for shipping (real)', \n                  'Days for shipment (scheduled)',\n                  'Customer Country',\n                  'Sales per customer',\n                  'Delivery Status', \n                  'Late_delivery_risk', \n                  'Customer City',\n                  'Customer Segment',\n                  'Sales','Shipping Mode',\n                  'Type', 'Product Card Id',\n                  'Customer Zipcode', \n                  'Product Category Id', \n                  'Product Name',                    \n                  'Product Price',\n                  'Market', \n                  'Product Status',\n                  'shipping date (DateOrders)',]\n\n# Specify the columns to drop\ncolumns_to_drop = [col for col in df.columns if col not in colums_to_keep ]\n\ndf = drop_columns(df, columns_to_drop)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Drop Rows with Missing Values","metadata":{}},{"cell_type":"code","source":"# drop customer Zip code.\ndf = df.drop(columns=['Customer Zipcode'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### A Quick Spot Check for Missing Values","metadata":{}},{"cell_type":"code","source":"### Check for Missing values\ndef check_null_values(df):\n    null_values = df.isnull().sum()\n    if null_values.sum() == 0:\n        print(\"No null values found ✅\")\n    else:\n        print(\"⚠️ Null values found in the following columns:\")\n        for column, null_count in null_values.iteritems():\n            if null_count > 0:\n                print(f\"{column}: {null_count}\")\n\n# Use the function\ncheck_null_values(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>In the code above, `df.isnull().sum()` returns a Series where the index is the column names and the values are the count of null values in each column. If the sum of these counts is 0, it means there are no null values in the DataFrame, so it prints \"No null values found\". Otherwise, it iterates over the Series and prints the column names and counts of null values.</p>","metadata":{}},{"cell_type":"markdown","source":"#### Create New Features\n<p>The dataset contains a `shipping date` column which is a `DateTime` object from which we can extract `Month`, `Year`, `Day` and `Day of Week` that can be useful in our analysis.</p>\n\n- [x] `Month` - to capture the months per sale.\n- [x] `Year` - to capture the year per sales.\n- [x] `Day` - to capture the day per sales.\n- [x] `Day of Week` - to capture the day of the week per sales.\n\n<p>\nwe need to also create a new  `Lead Time` column which is the difference between the `Days for shipment (scheduled)` and the `Days for shipping (real)`. This will help us understand the impact of lead time on demand.\n</p>","metadata":{}},{"cell_type":"code","source":"#| code-fold: true\n#| code-summary: \"Show the code\"\n\n# Create month, Year, Day, and Weekday columns from Shipping Date\ndef extract_date_parts(df, date_column, prefix):\n    try:\n        df[date_column] = pd.to_datetime(df[date_column])\n        df[f'{prefix} Year'] = df[date_column].dt.year\n        df[f'{prefix} Month'] = df[date_column].dt.month\n        df[f'{prefix} Day'] = df[date_column].dt.day\n        df[f'{prefix} Weekday'] = df[date_column].dt.weekday\n        # verify and notify that the columns have been created\n        if f'{prefix} Year' in df.columns and f'{prefix} Month' in df.columns and f'{prefix} Day' in df.columns and f'{prefix} Weekday' in df.columns:\n            print(f\"✅ Success! Columns Created: {prefix} Year, {prefix} Month, {prefix} Day, and {prefix} Weekday\")\n            return df\n        else:\n            print(\"Error creating columns. Please check that the date column name is correct.\")\n    except Exception as e:\n        print(f\"Error creating columns: {e}\")\n        return df\n# Add Lead Time Feature from Days for shipping (real) and Days for shipment (scheduled)\ndf['Lead Time'] = df['Days for shipping (real)'] - df['Days for shipment (scheduled)']\n\n# Use the function to extract date parts\ndf = extract_date_parts(df, 'shipping date (DateOrders)', 'Shipping')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display the shape of the data frame\ndf.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Now we have 23 columns and 180519 entries (rows) in the dataset.</p>","metadata":{}},{"cell_type":"markdown","source":"### Data Encoding\n<p>The nature of categorical data makes it unsuitable for future analysis. For instance, machine learning models can't work with categorical values for customer origins like `UK`, `USA`, `France`, etc. We will convert these categorical values to numerical values using the `LabelEncoder` from the `sklearn` library.</p>\n\n<p>I will also perform a <a href=\"https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/\"> one-hot encoding</a> technique on categorical features for future machine learning modeling tasks.</p>\n\n<p> I wrote a `prepare_data()` function that returns two preprocessed dataframes: one that is encoded using a label encoder function and the other encoded using one hot encoding technique.</p>\n\n<p>You can learn about encoding techniques for categorical variables <a href=\"https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/\">here</a></p>\n","metadata":{}},{"cell_type":"code","source":"#| code-fold: true\n#| code-summary: \"Show the code\"\n# Select top selling product\ntop_product = df['Product Card Id'].value_counts().index[0]\n# get top product ID\nprint(f\"Filtering and Encoding Dataset for Top Product ID: {top_product}\")\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndef prepare_data(df, product_card_id, categorical_cols, columns_to_drop):\n    \"\"\"\n    Prepare a DataFrame for bivariate analysis and machine learnin\n    g by applying label encoding and one-hot encoding to categorical \n    columns and dropping specified columns.\n\n    Parameters:\n    df (pandas.DataFrame): The original DataFrame.\n    product_card_id (int): The product card ID to filter the DataFrame on.\n    categorical_cols (list of str): The names of the categorical columns to apply encoding to.\n    columns_to_drop (list of str): The names of the columns to drop from the DataFrame.\n\n    Returns:\n    pandas.DataFrame: The label encoded DataFrame for bivariate analysis.\n    pandas.DataFrame: The one-hot encoded DataFrame for machine learning.\n    \"\"\"\n    try:\n        df_copy = df[df['Product Card Id'] == product_card_id].copy()  # create a copy\n\n        # label encoding\n        label_encoder = LabelEncoder()\n        df_label_encoded = df_copy.copy()\n\n        # Apply label encoding to categorical variables in place\n        for col in categorical_cols:\n            df_label_encoded[col] = label_encoder.fit_transform(df_label_encoded[col])\n\n        # Drop specified columns\n        df_label_encoded = df_label_encoded.drop(columns=columns_to_drop)\n\n        # one-hot encoding\n        df_one_hot_encoded = pd.get_dummies(df_copy, columns=categorical_cols)\n\n        # Drop specified columns\n        df_one_hot_encoded = df_one_hot_encoded.drop(columns=columns_to_drop)\n        print(\"Data Encoding successful. ✅\")\n        return  df_one_hot_encoded, df_label_encoded\n    except Exception as e:\n        print(f\"Error preparing data: {e}\")\n        return None, None\n\n# Use the function to prepare the data for bivariate analysis\ncategorical_cols = ['Type', 'Customer Segment', \n                    'Delivery Status', \n                    'Customer City', \n                    'Market',\n                    'Shipping Mode']\n\ncolumns_to_drop = ['Product Name',\n                   'Days for shipment (scheduled)', \n                   'Sales per customer', \n                   'Days for shipping (real)',\n                   'Customer Country', \n                   'shipping date (DateOrders)', \n                   'Product Card Id', \n                   'Product Category Id', \n                   'Product Status', \n                   'Product Price']\n\n# drop columns and encode data for correlation martrix and Machine learning\nonehot_encode_df, label_encode_df = prepare_data(df, top_product, categorical_cols, columns_to_drop)\n\n# rename Type column to Payment Type\nlabel_encode_df = label_encode_df.rename(columns={'Type': 'Payment Type'})\nonehot_encode_df = onehot_encode_df.rename(columns={'Type': 'Payment Type'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Confirm Encoding of Dataset ","metadata":{}},{"cell_type":"code","source":"label_encode_df.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validate the one-hot encoding\nonehot_encode_df.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p> The dataset is now ready for further analysis and modeling. we can now proceed to conduct exploratory data visualizations to understand the distribution of the data better.</p>","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Visualizations\n<p>To highlight the distributions of the individual variables as well as the relationship between the variables and the target variables, I used the following techniques:</p>\n\n- [x] Univariate Analysis\n- [x] Exploratory Time Series Analysis","metadata":{}},{"cell_type":"markdown","source":"### Univariate Analysis\n<p>Univariate analysis is the simplest form of data analysis where the data being analyzed contains only one variable. Since it's a single variable, it doesn't deal with causes or relationships. The main purpose of univariate analysis is to describe the data and find patterns that exist within it.</p>\n\n#### Visualizing the Distribution of the Dataset","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\nfig.suptitle('Distribution Plots for Selected Variables', \n             fontsize=16)\n# Create a copy of the DataFrame\ndf_copy = df.copy()\n\n\n# Plotting  the top ten products per Product Card Id\nsns.countplot(data=df_copy, x='Product Card Id',\n                color='blue', ax=axes[0, 0], \n                order=df_copy['Product Card Id'].value_counts().iloc[:10].index)\naxes[0, 0].set_title('Distribution of Top Ten Product Id')\naxes[0, 0].set_xlabel('Product Card Id')\naxes[0, 0].set_ylabel('Count')\n\n\n# Plotting Value of sales in  dollars\nsns.histplot(data=df_copy, x='Sales', \n             kde=True, color='salmon', \n             bins=30, linewidth=2,\n             ax=axes[0, 1])\naxes[0, 1].set_title('Distribution of Sales')\naxes[0, 1].set_xlabel('Sales value in Dollars')\naxes[0, 1].set_ylabel('Frequency')\n\n\n# Plotting Sales Value per customer\nsns.histplot(data=df_copy, x='Sales per customer',\n             bins=30, kde=True, linewidth=2,\n             color='lightblue', ax=axes[0, 2])\naxes[0, 2].set_title('Distribution of Sales per Customer')\naxes[0, 2].set_xlabel('Sales per Customer')\naxes[0, 2].set_ylabel('Frequency')\n\n# Ploting the distribution of Product Price\nsns.histplot(data=df_copy, x='Product Price', bins=30, kde=True, \n             color='lightgreen', linewidth=2, ax=axes[1, 0])\n\naxes[1, 0].set_title('Distribution of Product Price')\naxes[1, 0].set_xlabel('Product Price')\n\n# ploting a tree map for Customer Segment\nsquarify.plot(sizes=df_copy['Customer Segment'].value_counts(), \n              label=df_copy['Customer Segment'].value_counts().index, \n              color=sns.color_palette(\"Set3\"), ax=axes[1, 1])\naxes[1, 1].set_title('Distribution of Customer Segment - Treemap')\n\n# ploting a tree map for Top Ten Product Category Id\nsquarify.plot(sizes=df_copy['Product Category Id'].value_counts().iloc[:10],\n                label=df_copy['Product Category Id'].value_counts().iloc[:10].index,\n                color=sns.color_palette(\"Set2\"), ax=axes[1, 2])\naxes[1, 2].set_title('Distribution of Top Ten Product Category Id - Treemap')\n\n# Plotting the distribution of Delivery Status\nsns.countplot(data=df_copy, x='Delivery Status',\n                color='pink', ax=axes[2, 0])\naxes[2, 0].set_title('Distribution of Delivery Status')\naxes[2, 0].set_xlabel('Delivery Status')\naxes[2, 0].set_ylabel('Count')\n\n\n# Plotting the distribution Payment Type with stacked bar chart\ndf_copy.groupby(['Type'])['Type'].count().plot(kind='bar', \n                                               stacked=True,\n                                               ax=axes[2, 1])\n\naxes[2, 1].set_title('Distribution of Payment Type')\naxes[2, 1].set_xlabel('Payment Type')\naxes[2, 1].set_ylabel('Count')\n\n# Plotting the Distribution of top ten Customer Country\nsns.countplot(data=df_copy, x='Customer Country',\n                color='orange', ax=axes[2, 2], \n                order=df_copy['Customer Country'].value_counts().iloc[:10].index)\naxes[2, 2].set_title('Distribution of Customer Country')\naxes[2, 2].set_xlabel('Customer Country')\naxes[2, 2].set_ylabel('Count')\n\n\n\n# Adjust layout\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\n# Show the plots\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<details>\n<summary>\nClick to Read My Observations!\n</summary>\n<p>\nThe Top Selling <code>Product ID</code> is <code>365</code> which corresponds to a <code>product name</code>: <code>Perfect Fitness Perfect Rip Deck</code> this indicates a fast-moving product. I will focus the demand forecasting process on this product going forward\n</p>\n<p>\nThe distribution of <code>Sales Value</code> and <code>Sales per customer</code> are both positively skewed with a long tail. This indicates that the majority of sales are for low-value products. This is an interesting insight because it may suggest that the majority of customers are price-sensitive.\n</p>\n<p>\nThe distribution of <code>Product Price</code> is also positively skewed with a long tail. This means that the majority of products are low-value products.\n</p>\n<p>\nThe distribution of <code>Customer Segment</code> indicates that the majority of customers are from the consumer segment.\n</p>\n</details>","metadata":{}},{"cell_type":"markdown","source":"\n\n**NOTE**: Based on the insight from the univariate analysis, The rest of the analysis and forecasting will focus on the top-selling `Product Card Id` (365 ‘Perfect Fitness Perfect Rip Deck’)\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Time Series Visualisation\n<p>To understand the demand patterns of the top-selling product, let's create a time series heatmap to visualize the demand patterns of the top-selling product over time.</p>","metadata":{}},{"cell_type":"markdown","source":"### Time Series HeatMap of The Demand","metadata":{}},{"cell_type":"code","source":"#| code-fold: true\n#| code-summary: \"Show the code\"\t\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport calendar\n\n# Extract shipping date (DateOrders) and Sales columns\ndf_heatmap = df[['shipping date (DateOrders)', 'Sales']]\n# Assuming 'df' is your original dataframe\n\ndf_heatmap.set_index('shipping date (DateOrders)', inplace=True)\nresampled_df = df_heatmap.resample('M').sum()  # Resample to yearly frequency\n# Set x-axis ticks to represent months and years\nmonth_labels = [calendar.month_abbr[m.month] + '-' + str(m.year) for m in resampled_df.index]\n# Plot the heatmap\nplt.figure(figsize=(20, 10))\nsns.heatmap(resampled_df.T, cmap='YlGnBu', cbar_kws={'label': 'Sales'})\nplt.xticks(ticks=range(len(month_labels)), labels=month_labels, rotation=80, ha='right')\n\nplt.title('Time Series Heatmap of Sales (Aggregated by Month)')\nplt.xlabel('Month and Year')\n\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Judging from consistency in the shades of the heatmap, we can see that the demand for the top-selling product is fairly stable over time. However, it is interesting to note that the number of sales recorded for the first quarters of <code>2015</code>, <code>2016</code> and <code>2017</code> remained consistent however in 2018 the number of sales recorded in the first quarter dipped significantly. This is an interesting insight that we can explore further.\n</p>\n<p>\nNext, Let's use the <code>Prophet</code> library to model the demand for the top-selling product. This will help us understand the cyclical patterns in the demand for the top-selling product\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"\n<h1>\nForecasting Demand with Prophet\n</h1>\n<p>\nProphet is a forecasting tool developed by Facebook. It is designed for analyzing time series data that display patterns on different time scales such as yearly, weekly, and daily. It also has advanced capabilities for modeling the effects of holidays on a time series and implementing custom seasonalities. see the documentation <a href=\"https://facebook.github.io/prophet/docs/quick_start.html\">here</a>\n</p>","metadata":{}},{"cell_type":"code","source":"#| code-fold: true\n#| code-summary: \"Show the code\"\n\n# import prophet\nfrom prophet import Prophet\n\nprophet_df = df.copy()\nprophet_df = prophet_df.rename(columns={'shipping date (DateOrders)': 'ds', 'Sales': 'y'})\n# Add custom Puerto Rico holidays\n# Read the CSV file\nholidays_df = pd.read_csv('data/puertorican_holidays.csv')\n\n# Rename the 'Date' column to 'ds' and the 'Name' column to 'holiday'\nholidays_df = holidays_df.rename(columns={'Date': 'ds', 'Name': 'holiday'})\n\n# Drop the 'Type' column as it's not needed\nholidays_df = holidays_df.drop(columns=['Type'])\n\n# Add 'lower_window' and 'upper_window' columns\nholidays_df['lower_window'] = 0\nholidays_df['upper_window'] = 1\n\n# Convert 'ds' to DateTime\nholidays_df['ds'] = pd.to_datetime(holidays_df['ds'])\n\n# Create a Prophet instance and provide the holidays DataFrame\nprophet = Prophet(holidays=holidays_df)\n\nprophet.fit(prophet_df)\n\n# Create a DataFrame with future dates for forecasting\nfuture = prophet.make_future_dataframe(periods=365, freq='D')\n\n# Generate forecasts\nforecast = prophet.predict(future) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\nThe code above uses the <code>Prophet</code> library to model the demand for the top-selling product. The model is trained on the <code>Sales</code> and <code>Shipping Date</code> columns. The model is then used to forecast the demand for the top-selling product over the next 365 days.\n</p>\n<p>\nThe code also included Puerto Rican holidays to account for the impact of holidays on the demand for the top-selling product. This is important because holidays can have a significant impact on demand patterns.\n</p>\n<p>\nYou might wonder why Puerto Rican holidays were included in the model. From the univariate analysis conducted earlier, we discovered that most of the orders were coming from Puerto Rico. The <code>forecast</code> variable now contains the forecasted values for the top-selling product. we will work with the variable later but for now, let’s evaluate the accuracy of our prophet model\n</p>","metadata":{}},{"cell_type":"markdown","source":"<h3>Evaluating the Accuracy of the Time Series Forecast</h3>","metadata":{}},{"cell_type":"markdown","source":"<p>To determine the accuracy of the prophet model, we will use the `cross_validationa()` function provided by `Prophet`</p>","metadata":{}},{"cell_type":"code","source":"from prophet.diagnostics import cross_validation, performance_metrics\n# Perform cross-validation\ndf_cv = cross_validation(model=prophet, initial='730 days', period='365 days', horizon='365 days')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\nThe <code>cross_validation()</code> function performs cross-validation on the model. It trains the model on a subset of the data and then evaluates the model on the remaining data. This is a good way to evaluate the accuracy of the model. The <code>initial</code> parameter specifies the size of the training set. The <code>period</code> parameter specifies the frequency of the forecast.\n</p>\n<p>\nLet’s visualize the performance of the model\n</p>","metadata":{}},{"cell_type":"code","source":"# Plot MAPE\nfrom prophet.plot import plot_cross_validation_metric\n#  set fig size\nplt.figure(figsize=(9, 6))\nfig = plot_cross_validation_metric(df_cv, metric='mape')\nfig.get_axes()[0].get_lines()[0].set_markerfacecolor('#ADD8E7')  # Change color of the dots\nfig.get_axes()[0].get_lines()[0].set_markeredgecolor('#ADD8E7')  # Change color of the dot edges","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\nThe forecast has lower `MAPE` (Mean Absolute Percentage Error) values for horizons within the 200-day range however the accuracy drops for horizons beyond 250 days. This suggests that the model is making more errors at periods beyond 250 days.\n</p>\n<p>\nThe model will be most useful to stakeholders if it can forecast demand beyond 250 days with a lower percentage of errors. Exposing the model to more historical data may help lower the MAPE significantly. nonetheless, let’s explore if there are opportunities to improve the accuracy by finding the best combinations of hyperparameters for the prophet model. I will use a <a href=\"https://aws.amazon.com/what-is/hyperparameter-tuning/#:~:text=Hyperparameter%20tuning%20allows%20data%20scientists,the%20model%20as%20a%20hyperparameter.\">hyperparameter tuning</a> technique to try to optimize the model’s performance.\n</p>\n<h4 class=\"anchored\">\nFinding the Best Hyperparameter Combination for Lower MAPE\n</h4>","metadata":{}},{"cell_type":"code","source":"#| code-fold: true\n#| code-summary: \"Show the code\"\nfrom sklearn.model_selection import ParameterGrid\n\n# Assuming prophet_df is your DataFrame with 'ds' and 'y' columns\nprophet_df = df.copy()\nprophet_df = prophet_df.rename(columns={'shipping date (DateOrders)': 'ds', 'Sales': 'y'})\n\n# Specify hyperparameter values to try\nparam_grid = {\n    'seasonality_mode': [\"additive\", 'multiplicative'],\n    'seasonality_prior_scale': [1, 5, 10, 20],\n    'holidays_prior_scale': [5, 10, 20, 25],\n    'changepoint_prior_scale': [0.005, 0.01, 0.05, 0.1]\n}\n\n# Generate all combinations of hyperparameters using ParameterGrid\nparam_combinations = ParameterGrid(param_grid)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\nThe code above uses the <code>ParameterGrid</code> function from the <code>sklearn</code> library to create a grid of hyperparameters. The grid contains different combinations of hyperparameters for the prophet model.\n</p><p>\nOn the other hand, the code below uses the <code>cross_validation()</code> function to evaluate the accuracy of the model for each combination of hyperparameters. The code then selects the combination of hyperparameters that results in the lowest MAPE.\n</p>","metadata":{}},{"cell_type":"code","source":"#| code-fold: true\n#| code-summary: \"Show the code\"\nfrom itertools import product\n# Store results in a dictionary\nresults = {}\nprint(f\"trying all {len(param_combinations)} hyperparameter combinations\")\n# Generate all combinations of hyperparameters\nparam_combinations = list(product(*param_grid.values()))\n\nfor params in param_combinations:\n    # Create a Prophet instance with current hyperparameter values\n    prophet = Prophet(**dict(zip(param_grid.keys(), params)))\n\n    # Fit the model\n    prophet.fit(prophet_df)\n\n    # Perform cross-validation\n    df_cv = cross_validation(model=prophet, initial='730 days', period='365 days', horizon='365 days')\n\n\n    # Calculate performance metrics\n    df_metrics = performance_metrics(df_cv, rolling_window=0)\n\n    # Store metrics in the results dictionary\n    results[params] = df_metrics['mape'].mean()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":":::{.callout .note}\n\n**NOTE**: The code took a very long time to complete. It tried 128 different combinations of hyperparameters and the best model was the one with the lowest MAPE value.\n::: ","metadata":{}},{"cell_type":"markdown","source":"<p>\nThe results are in! The best model had the following hyperparameters:\n</p>","metadata":{}},{"cell_type":"code","source":"# Find the hyperparameters with the lowest RMSE\nbest_hyperparams = min(results, key=results.get)\nprint(f\"Best Hyperparameters: {dict(zip(param_grid.keys(), best_hyperparams))}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"cell-output cell-output-stdout\">\n<pre><code>Best Hyperparameters: {'seasonality_mode': 'additive', 'seasonality_prior_scale': 1, 'holidays_prior_scale': 5, 'changepoint_prior_scale': 0.005}</code></pre>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<p>Now let’s rebuild the model with the best hyperparameters and evaluate the model’s performance.</p>","metadata":{}},{"cell_type":"code","source":"tuned_prophet = Prophet(holidays=holidays_df, \n                        seasonality_mode='additive', \n                        seasonality_prior_scale=1, \n                        holidays_prior_scale=5, \n                        changepoint_prior_scale=0.005)\n# fit the model\ntuned_prophet.fit(prophet_df)\n# Create a DataFrame with future dates for forecasting\nfuture = tuned_prophet.make_future_dataframe(periods=365, freq='D')\n\n# Generate forecasts\nnew_forecast = tuned_prophet.predict(future)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>Cross Validation of the Best Model</h5>","metadata":{}},{"cell_type":"code","source":"from prophet.diagnostics import cross_validation, performance_metrics\n# Perform cross-validation\ntuned_df_cv = cross_validation(model=tuned_prophet, initial='730 days', period='365 days', horizon='365 days')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>let’s compare the accuracy of the model before and after hyperparameter tuning.</p>","metadata":{}},{"cell_type":"code","source":"#| code-fold: true\n#| code-summary: \"Show the code\"\nfig, axs = plt.subplots(1, 2, figsize=(15, 9))\n\n# Plot the first cross-validation metric\nfig1 = plot_cross_validation_metric(df_cv, metric='mape', ax=axs[0])\nfig1.get_axes()[0].get_lines()[0].set_markerfacecolor('#ADD8E7')  # Change color of the dots\nfig1.get_axes()[0].get_lines()[0].set_markeredgecolor('#ADD8E7')  # Change color of the dot edges\n# add title\naxs[0].set_title('Initial Cross-Validation score MAPE')\n\n# Plot the second cross-validation metric\nfig2 = plot_cross_validation_metric(tuned_df_cv, metric='mape', ax=axs[1])\nfig2.get_axes()[0].get_lines()[0].set_markerfacecolor('#ADD8E7')  # Change color of the dots\nfig2.get_axes()[0].get_lines()[0].set_markeredgecolor('#ADD8E9')  # Change color of the dot edges\n# add title\naxs[1].set_title('Tuned Cross-Validation score MAPE')\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Not Exactly the outcome I was expecting but the tuned model’s performance remains consistent with the previous model. This may suggest that the model is not sensitive to the hyperparameters. Nonetheless, the model is still useful for forecasting demand for the top-selling product.</p>","metadata":{}},{"cell_type":"markdown","source":"<h4 class=\"anchored\">\nForecast Results\n</h4>\n<p>As indicated earlier, the <code>forecast</code> variable contains the forecasted values of our <code>Sales</code> time series. Based on this forecast we will calculate the optimal inventory policy for this specific product.</p>\n<p>\nThe <code>forecast</code> variable is a dataframe that contains the following columns:\n</p>","metadata":{}},{"cell_type":"code","source":"forecast.head(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Before calculating the optimal inventory policy, let’s visualize the forecasted sales data. To have a feel for the seasonalities and cycles in the forecasted sales data</p>","metadata":{}},{"cell_type":"markdown","source":"<h4 class=\"anchored\">\nVisualizing Forecasted Sales\n</h4>","metadata":{}},{"cell_type":"code","source":"import warnings\n\n# Ignore the specific FutureWarning\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Plot the forecast\ntuned_prophet.plot_components(new_forecast)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 class=\"anchored\">\n Business Question #1 \n</h1>\n","metadata":{}},{"cell_type":"markdown","source":"\n<div style=\"text-align:center;\">\n<img src=\"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExeWJlamcxOTJnM3ZyeDllcXE4MW1lOHpmMndrMnJpMms0YnZ4aWNwcyZlcD12MV9naWZzX3NlYXJjaCZjdD1n/xT5LMWNOjGqJzUfyve/giphy.gif\" alt=\"gif\" style=\"width: 70%; height: 70%; display: block; margin: auto; right:20px;\">\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"\n><em><h5>What is the demand forecast for the top selling product in the next 24 months?</h5></em>\n","metadata":{}},{"cell_type":"markdown","source":"\n<p>\n<p>The sales trend between <code>2015</code> and <code>2017</code> marks a cycle where sales for the product remained relatively stable during the second and third quarters of each year and then dipped slightly in October with a sharp increase between November and December.\n</p>\n<p>The forecasted sales for the next 24 months, on the other hand, indicate a very stable demand pattern.\n<p>\nThe zero variance observed in the product price may account for the relatively stable sales pattern forecasted for `2018` and `2019`. It might also be worth investigating the factors that may account for the cyclical dips between 2015 and 2017\n</p>\n<p>\nWe can also observe the impact of the Puerto Rican holidays on the forecasted sales.\n</p>\n<h1 class=\"anchored\">\nFinding Optimal Inventory Policy Based on Forecasted Demand\n</h1>\n<p>\nNow that we have forecasted the demand for the top-selling product, we can use the forecasted demand to calculate the optimal inventory policy for the product.</p>\n<p> \nFinding the optimal inventory policy will help us determine the <a href= \"https://cashflowinventory.com/blog/reorder-point\">Reorder Point</a>, <a href=\"https://cashflowinventory.com/blog/reorder-point\">safety stock</a>, and <a href=\"https://cashflowinventory.com/blog/reorder-point\" >Economic Order Quantity</a>(EOQ) for the product. These markers will help us ensure that we have the right amount of inventory on hand to meet customer demand while minimizing inventory costs.\n</p>","metadata":{}},{"cell_type":"markdown","source":"<h2>Re Order Point</h2>\nThe reorder point is the inventory level at which we should reorder more stock. \n`ROP`  is calculated as the product of the average sales per day and the lead time (also referred to as `Lead Time Demand`) plus the `Safety stock`.","metadata":{}},{"cell_type":"markdown","source":"$$ Reorder\\ Point = \\text{Lead\\ Time\\ Demand} + \\text{Safety\\ Stock} $$\n","metadata":{}},{"cell_type":"markdown","source":"let's Find the Lead Time Demand\n\n$$ \\text{Lead Time Demand} = \\text{Average Sales Per Day} \\times \\text{Lead Time} $$","metadata":{}},{"cell_type":"markdown","source":":::{.callout .note}\n\n<p> <strong>NOTE:</strong> It is important to state the difference between the downstream and upstream lead time.</p>\n<p>The `Lead Time` with regards to the reordering point is the time it takes for a product to be delivered by upstream supply chain manufacturers to the store's warehouse. It is the time between the placement of an order by the store and the receipt of the product.</p>\n<p> The current dataset only provides the `Days for shipment (scheduled)` and the `Days for shipping (real)` which only helps us determine downstream lead times. The downstream lead time is the time it takes for a product to be delivered to the customer after it has been ordered. This is not the lead time we need to calculate the reorder point.</p>\n\n:::\n\n","metadata":{}},{"cell_type":"markdown","source":"<p>For the purposes of our analysis we will assume an average upstream `Lead time` of 7 days </p>","metadata":{}},{"cell_type":"code","source":"\n# Extract average forecasted sales per day\naverage_forecasted_sales = new_forecast['yhat'].mean()\n\n\n# Extract the average lead time\naverage_lead_time = 7  # 7 days\nprint(f\"Average Lead Time: {average_lead_time}\")\n\nlead_time_demand = average_forecasted_sales * average_lead_time\nprint(f\"Lead Time Demand: {lead_time_demand}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>One final piece to the Reorder Point puzzle is the ` Safety Stock`. The safety stock is the extra stock that is kept on hand to mitigate the risk of stockouts due to uncertainties in demand and lead time.</p>","metadata":{}},{"cell_type":"markdown","source":"$$ \\text{Safety Stock} = (\\text{Maximum Daily Sales} \\times \\text{Maximum Lead Time}) - \\text{Lead Time Demand}$$\n","metadata":{}},{"cell_type":"markdown","source":"<p>Let's also assume that there have been delays from the manufacturer in the past and the maximum lead time is 10 days. That is Three days later than the average order fulfillment timeline</p>","metadata":{}},{"cell_type":"code","source":"# find maximum daily forecasted sales\nmax_daily_forecasted_sales = new_forecast['yhat'].max()\nprint(f\"Maximum Daily Forecasted Sales: {max_daily_forecasted_sales}\")\n\n# find maximum lead time\nmax_lead_time = average_lead_time + 3  # 3 days delays in delivery than the average\nprint(f\"Maximum Lead Time: {max_lead_time}\")\n\n# calculate safety stock\nsafety_stock = (max_daily_forecasted_sales * max_lead_time) - lead_time_demand\nprint(f\"Safety Stock: {safety_stock}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Finally, we can calculate the reorder point for the top-selling product.</p>\n\n<h3>Putting It All Together</h3>","metadata":{}},{"cell_type":"code","source":"# calculate reorder point\nreorder_point = lead_time_demand + safety_stock\nprint(f\"The Optimal Reorder Point for the Top-selling Product is: {reorder_point}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p> As indicated by the result, the reorder point for the top-selling product is `3753` units, which means that we should reorder more stock when the inventory level reaches `3753` units. This will help us ensure that we have enough stock on hand to meet customer demand while minimizing inventory costs.</p>","metadata":{}},{"cell_type":"markdown","source":"<h3>Economic Order Quantity (EOQ)</h3>\n<p> Alternatively, we can use the <a href=\"https://dclcorp.com/blog/inventory/economic-order-quantity-eoq/\">Economic Order Quantity (EOQ) model</a> to calculate the optimal order quantity for the top-selling product. The EOQ model helps us determine the optimal order quantity that minimizes the total inventory costs.</p>\n\n<p>Unlike the Reorder Point which is concerned with determining the level of inventory at which a new order should be placed to avoid stockouts, EOQ takes into account the costs of ordering (e.g., setup costs) and the costs, of holding inventory (e.g., storage costs, opportunity costs).</p>\n\n","metadata":{}},{"cell_type":"markdown","source":"<p>The Economic Order Quantity (EOQ) formula is given by:</p>\n\n$$ EOQ = \\sqrt{\\frac{2DS}{H}} $$\n\n<p>Where:</p>\n\n- \\( D \\) is the demand rate (number of units demanded),\n- \\( S \\) is the ordering cost per order,\n- \\( H \\) is the holding cost per unit per year.\n\n<p>This formula helps in determining the optimal order quantity that minimizes the total inventory costs.</p>\n","metadata":{}},{"cell_type":"markdown","source":":::{.callout .note}\n**NOTE**: We can figure out the demand rate \\( D \\) based on the existing data. However, the ordering cost \\( S \\)  and holding cost \\( H \\)  are not provided in the dataset. For our analysis, We will assume that the ordering and holding cost is `10%` and `30%` of the product price respectively.\n\n:::\n","metadata":{}},{"cell_type":"markdown","source":"<h4>Estimating Holding Cost and Ordering Cost</h4>\n<h5>Holding Cost</h5>\n<p>Holding costs typically include expenses related to storing inventory, such as warehousing, insurance, and security. It also includes the opportunity cost of tying up capital in inventory.</p> \n\n<h5>Ordering Cost</h5>\n<p>Ordering costs are the expenses incurred in the process of ordering and receiving inventory. This includes the cost of preparing and processing purchase orders, receiving and inspecting the goods, and storing and managing the inventory.</p>","metadata":{}},{"cell_type":"code","source":"# extract the product price of top selling product(product card id:365)\nproduct_price = df[df['Product Card Id'] == 365]['Product Price'].iloc[0]\nprint(f\"The Product Price is: {product_price}\")\n# calculate holding cost\nH = 0.10 * product_price\n# calculate ordering cost\nS = 0.30 * product_price\n\n# calculate forecasted demand rate\nD = new_forecast['yhat'].mean()\n\nprint(f\"The Demand Rate is: {D}\")\nprint(f\"The Holding Cost is: {H}\")\nprint(f\"The Ordering Cost is: {S}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Putting It All Together</h4>","metadata":{}},{"cell_type":"code","source":"EOQ = math.sqrt((2 * D * S) / H)\nprint(f\"The Economic Order Quantity is: {EOQ}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Based on the EOQ model, the optimal order quantity for the top-selling product is `35` units. This means that we should order `35` units of the product at a time to minimize the total inventory costs. This will help us ensure that we have the right amount of inventory on hand to meet customer demand while minimizing inventory costs.</p>","metadata":{}},{"cell_type":"markdown","source":"<h1 class=\"anchored\">\n Business Question #2 \n</h1>","metadata":{}},{"cell_type":"markdown","source":"\n\n<div style=\"text-align:center;\">\n<img src=\"https://media.giphy.com/media/l4FBeaUCLhxrXPlpC/giphy.gif\" alt=\"gif\" style=\"width: 90%; height: 90%; display: block; margin: auto;\">\n</div>","metadata":{}},{"cell_type":"markdown","source":"See Angry Jack(from spongebob) being angry about his Massive inventory buildup. soon we will see how he could use our new inventory optimisations.","metadata":{}},{"cell_type":"markdown","source":"><em><h5>What is the optimal inventory level for the top selling product?</h5></em>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"card\">\n<p>The optimal inventory policy for the top-selling product is as follows:</p>\n\n- **Reorder Point**: 3753 units\n- **Economic Order Quantity (EOQ)**: 35 units\n- **Safety Stock**: 2284 units\n\n<p>Here's what Angry Jack should do: \n</p>\n<p>When the stock level of the top-selling product hits 3753 units, He needs to place an order of 35 units with his suppliers.</p>\n<p>Inventory management decisions based on these markers will help the company ensure that there is the right amount of inventory of the top-selling product on hand to meet customer demand while minimizing inventory costs.</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h1>Bonus: Investigating the Top Predictors of Demand Outcomes</h1>\n<p>Now that we have forecasted the demand for the top-selling product and calculated the optimal inventory policy, we can investigate the top predictors of demand outcomes. This will help us understand the factors that contribute to demand patterns and identify opportunities to improve sales outcomes.</p>\n\n<p>The top-selling product price has remained fixed throughout the dataset. We can safely rule out the product price as a predictor of demand outcome for the period under review.</p>","metadata":{}},{"cell_type":"markdown","source":"<h3>Wait....I have a Hypothesis</h3>\n<p>I have a hunch that `Product Lead Time`, `Customer Segment` and some `Geographic factors` will be top predictors of demand outcomes.</p>\n<p>I will test this hypothesis by conducting a feature importance analysis using the <a href=\"https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/\">Random Forest</a> algorithm.</p>","metadata":{}},{"cell_type":"markdown","source":"<h5><em>Let's Investigate.....</em></h5>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align:center;\">\n<img src=\"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExaTUzaDNxOW54cDU5MXl0emd3dzI4dnRqNjJpMWxpYW1sOGVrb2xjOSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/dgEtXsuqq2PL2/giphy.gif\" alt=\"gif\" style=\"width: 60%; height: 90%; display: block; margin: auto;\">\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h4>Correlation Analysis</h4>\n<p>Before we proceed with the feature importance analysis, let’s conduct a correlation analysis to identify the top predictors of demand outcomes.</p>\n\n<p>\nwe will create the <a href=\"https://builtin.com/data-science/correlation-matrix#:~:text=A%20correlation%20matrix%20is%20a,1%20a%20not%20strong%20relationship.\">correlation matrix</a> using the `corr()` method and then use the `heatmap()` function from the `seaborn` library to visualize the correlation matrix.</p>\n<p>We will use the `label_encode_df` data frame created during the data preprocessing stage.</p>\n\n","metadata":{}},{"cell_type":"code","source":"# correlation analysis\ncorrelation_matrix = label_encode_df.corr()\n\n# plot the correlation matrix\nplt.figure(figsize=(12, 10))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'})\nplt.title('Correlation Matrix of Selected Variables')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>Observation</h5>\n<p>The Heatmap shows there are no strong correlations between the features and the target variable. This suggests that the demand for the top-selling product is not strongly influenced by any single feature.</p>\n\n<h4>Feature Importance Analysis Using Random Forest Regressor Model</h4>\n<p>The correlation matrix could not identify the top predictors of demand outcomes. We will use the Random Forest algorithm to identify the top predictors of demand outcomes. The Random Forest algorithm is an ensemble learning method that uses multiple decision trees to make predictions. It is a powerful algorithm for feature importance analysis.</p>\n<p> We will train a Random Forest Regressor model on the `onehot_encode_df` data frame and then use the `feature_importances_` attribute of the model to identify the top predictors of demand outcomes.</p>","metadata":{}},{"cell_type":"markdown","source":"<h5>Split the Data for Model Training</h5>","metadata":{}},{"cell_type":"code","source":"#| code-fold: true\n#| code-summary: \"Show the code\"\n\nfrom sklearn.model_selection import train_test_split\n\n# prepare features excluding the sale\nX_features = onehot_encode_df.drop(columns=['Sales'])\n#  Drop Shipping year\nX_features = X_features.drop(columns=['Shipping Year'])\n# prepare target variable\ny_target = onehot_encode_df['Sales']\n\n# split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=42)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>Train the Random Forest Regressor Model</h5>","metadata":{}},{"cell_type":"code","source":"#| code-fold: true\n#| code-summary: \"Show the code\"\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# create a random forest regressor model\nrf_model = RandomForestRegressor(n_estimators=150, max_depth=10,  min_samples_split=2)\n\n# fit rf odel to the training data\nrf_model.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Feature Importance Analysis</h3>","metadata":{}},{"cell_type":"code","source":"#| code-fold: true\n#| code-summary: \"Show the code\"\nimport matplotlib.cm as cm\n# retrieve the feature importances\nfeature_importances = rf_model.feature_importances_\n\n# get the top 5 feature importances\ntop_5_feature_importances = feature_importances.argsort()[-5:]\n\n# visualize the top 5 features of importance\nplt.figure(figsize=(10, 6))\ncolors = cm.viridis(np.linspace(0, 1, \n                                len(top_5_feature_importances)))\nplt.barh(X_train.columns[top_5_feature_importances], \n         feature_importances[top_5_feature_importances],\n          color=colors)\nplt.xlabel('Feature Importance')\n\nplt.title('Top 5 Feature Importances')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Discussion of Results</h4>","metadata":{}},{"cell_type":"markdown","source":"<p>The feature importance analysis confirms the majority of my hypothesis</p>\n<p>We can see that Product `Lead Time`, and some `Geographic factors` were among the top predictors of demand outcomes</p>\n<p>`Customer Segment` however was not among the top predictors of demand outcomes. This brings us to the end of our analysis</p>","metadata":{}},{"cell_type":"markdown","source":"\n\n# **Congratulations on making it This Far!**\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align:center;\">\n<img src=\"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExb3V2N25lNWFrNGtwNGo5aTMxaWM2Yno0dXRmOHhtM2FvbnE0eW41eiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9cw/XEIa9GtOXdMN4KOaPn/giphy.gif\" alt=\"gif\" style=\"width: 100%; height: 1o0%; display: block; margin: auto;\">\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h1>Key Takeaways</h1>\n<p>If you made it this far, I hope you have enjoyed the journey. Here are the key takeaways from this project:</p>\n\n- [x] We explored how to build a demand forecasting model with Python.\n- [x] We used the model for inventory optimization, covering concepts like reorder points, safety stock, and economic order quantity (EOQ).\n- [x] We discovered trends, seasonalities and holiday effects on the top-selling product based on the dataset provided.\n- [x] We found the optimal inventory policy for the top-selling product.\n\n<h4>How does this method offer enhanced scalability compared to conventional demand forecasting techniques?</h4>\n\n<ol><li>The existing data processing steps can easily be adapted to accommodate larger product categories.\n<li>Improved speed and accuracy.</li>\n<li>Minimized guesswork and enhanced reproducibility for validating forecasts.</li>\n</ol>\n<p>Feel free to share your thoughts and feedback on my approach. I am open to learning and improving my skills.</p>","metadata":{}},{"cell_type":"markdown","source":"\n<h1>Up Next:</h1>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"display:flex; gap:50px;\">\n<a href=\"https://simontagbor.github.io/BritishAirways-data-science/\" style=\"text-decoration: none; color: black;\">\n<div style=\"width: 300px; border-radius: 15px; overflow: hidden; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); transition: transform 0.3s;\">\n  <img src=\"https://images.unsplash.com/photo-1615558568469-de10704d9102?q=80&w=1374&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\" alt=\"Blog Post Image\" style=\"width: 100%; height: auto; border-radius: 15px 15px 0 0;\">\n  <div style=\"padding: 15px;\">\n  <div style=\"font-size: 1.2em; font-weight: bold; margin-bottom: 10px;\">Understanding How Customers Feel About British Airways Flight Experience.</div>\n  <div style=\"font-size: 0.9em; color: #555;\">A Topic Modelling and Sentiment Analysis Approach</div>\n  </div>\n</div>\n</a>\n<a href=\"#\" style=\"text-decoration: none; color: black;\">\n<div style=\"width: 300px; border-radius: 15px; overflow: hidden; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); transition: transform 0.3s;\">\n  <img src=\"https://images.unsplash.com/photo-1614028674026-a65e31bfd27c?q=80&w=1470&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\" alt=\"Blog Post Image\" style=\"width: 100%; height: auto; border-radius: 15px 15px 0 0;\">\n  <div style=\"padding: 15px;\">\n  <div style=\"font-size: 1.2em; font-weight: bold; margin-bottom: 10px;\">Supply Chain Demand Forecasting Using Prohet</div>\n  <div style=\"font-size: 0.9em; color: #555;\">Using  Prophet Algorithm to Implement a Demand Forecasting Model with Python(Currently A Work In Progress)</div>\n  </div>\n</div>\n</a>\n<a href=\"#\"  style=\"text-decoration: none; color: black;\">\n<div style=\"width: 300px; border-radius: 15px; margin-right:15px; overflow: hidden; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); transition: transform 0.3s;\">\n  <img src=\"https://images.unsplash.com/photo-1616401784845-180882ba9ba8?q=80&w=1470&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\" alt=\"Blog Post Image\" style=\"width: 100%; height: auto; border-radius: 15px 15px 0 0;\">\n  <div style=\"padding: 15px;\">\n  <div style=\"font-size: 1.2em; font-weight: bold; margin-bottom: 10px;\">Inventory Optimisation With ARIMA and SARIMA</div>\n  <div style=\"font-size: 0.9em; color: #555;\">Using Time Series Analysis to forecast the future demand of a given product.(Work in Progress)</div>\n  </div>\n</div>\n</a>\n\n</div>","metadata":{}}]}